\chapter{Background}

In this section, we discuss the background for the project and provide context for the work performed.

\section{Plan Recognition}

Plan Recognition looks at a sequence of actions by an agent, and attempts to deduce that agent's goals. This can be seen as the inverse of Planning, where given a goal the problem is to deduce a sequence of actions which achieve it. 

There are many possible situations where it is desirable to recognise the intentions of an agent from its observed actions. In network intrusion detection, we wish to identify any actions which could correspond to an attack, hidden in a large number of benign actions by users, in real time. [TODO: cite ?] In human-robot interaction, we wish a human and a robot to work together on a task. If the robot partner can observe the actions of the human partner, and from them understand what the human partner is currently trying to achieve, it an anticipate the needs of the human partner, and assist or offer advice. [TODO: cite JAST project]. In assisted care scenarios, a robot can again anticipate the needs of the human being assisted; it can also recognise unusual behaviour which would necessitate emergency response. [TODO: cite ?]

Since Plan Recognition takes symbolic input, some amount of preprocessing of the input may be required. For network intrusion detection, this could be a simple filter on system logs. For more complex scenarios, Plan Recognition could tie in to a system which takes video and other real-time feeds and classifies the actions taken by humans using machine learning techniques.

Some Plan Recognition techniques use a formal grammar to define actions and plans to be recognised. This grammar could be learned or crafted by the designer; in this project, we use two grammars crafted by hand.

\section{Combinatory Categorial Grammars}

The problem of plan recognition is very similar to the problem of parsing, where we analyse a sequence of tokens and produce a syntax tree according to a formal grammar; so it is natural to consider the applicability of parsing techniques to this problem.

When parsing computer languages such as XML or C++, ambiguity is undesirable. An input text should admit to either a single interpretation or no interpretation at all, and it is the task of the language designer to resolve any possible ambiguities in the grammar.

% When parsing natural languages such as English, a degree of ambiguity is unavoidable; in fact, such ambiguity can be exploited by speakers, for example in puns. Correspondingly, grammars and parsers for natural language must deal with potential ambiguity and multiple possible parses.
When parsing a sequence of actions into possible explanations, a degree of ambiguity is unavoidable; for example, if we observe a truck driving from Edinburgh to Newcastle, it may be making a delivery locally in Newcastle; or it may be driving there in its way to York. Grammars and parsers for natural language must deal with potential ambiguity and multiple possible parses in human language. Therefore, we can expect parsing techniques from natural language processing may be more amenable to being adapted to the problem of Plan Recognition than parsing techniques for computer languages.

% TODO used to say ``directly'' to their left or right - still want to make that distinction?

Categorial Grammars assign \emph{categories} to each input token or symbol, and combine them according to simple rules. \emph{Basic} categories correspond to lexical classes such as ``identifier'' in a computer language or ``noun'' in a natural language. We denote basic categories by atomic terms such as $A$. \emph{Complex} categories are akin to functions over categories: they look for arguments to their left (denoted $\backslash$) or right (denoted $/$) in the sequence of categories, and yield other categories as a result. A complex category taking an argument $A$ to its left and yielding $B$ is denoted $B\backslash A$; one taking the same argument to the right is denoted $B/A$. [TODO footnote: some sources denote the leftward example above as $A\backslash B$, preserving the ordering of the argument and result. For consistency we will always denote the arguments on the right and the results on the left.]

Taking a simple example in English:\\

[TODO: replace with an example parse from the XPERIENCE or logistics domains]\\

English sentence: ``the bad boy made that mess''

Basic categories:\\
N - noun\\
NP - noun phrase\\
S - sentence\\

We then assign basic or complex categories to each word:

$$
{\text{the}\atop {NP/N,}}
{\text{bad}\atop {N/N,}}
{\text{boy}\atop {N,}}
{\text{made}\atop {(S \backslash NP)/NP,}}
{\text{that}\atop {NP/N,}}
{\text{mess}\atop {N}}
$$

We can then combine the complex categories with their arguments and produce a parse. Combining the categories follows the rules: $ X\leftarrow X/Y,\; Y$ and $ X\leftarrow Y,\; X\backslash Y$:\\
$.\qquad NP/N,\; N/N,\; N,\; (S\backslash NP)/NP,\; \underbrace{NP/N,\; N}$\\
$.\qquad NP/N,\; N/N,\; N,\; \underbrace{(S\backslash NP)/NP, \quad NP}$\\
$.\qquad NP/N,\; \underbrace{N/N,\; N}, \qquad (S\backslash NP)$\\
$.\qquad \underbrace{NP/N,\; \quad N},\; \qquad (S\backslash NP)$\\
$.\qquad \qquad\underbrace{NP,\; \qquad (S\backslash NP)}$\\
$.\qquad \qquad\qquad\quad\;\;\; S$


\emph{Combinatory Categorial Grammars} (CCGs) extend categorial grammars with new reductions corresponding to combinators in combinatory logic. Since complex categories can take arguments either to their left or right, each combinator has a leftward and a rightward version.

The leftward and rightward application combinators are the reductions already present in categorial grammars. The other two combinators commonly used in CCGs are composition and type-raising.

Composition corresponds to function composition. The rightward composition operation has the form: $X/Z \leftarrow X/Y,\;  Y/Z$. TODO: more explanation

Type-raising transforms basic categories to complex categories. The rightward type-raising operation has the form: $T/(T\backslash X) \leftarrow X$. TODO: more explanation

\section{Applying CCGs to Plan Recognition}

In applying \ac{ccg}s to Plan Recognition, we seek to assign (basic or complex) categories to the input sequence of actions, and by reductions using a set of combinators, produce a set of explanations for the input. Adjacency of grammar elements is not as crucial in Plan Recognition as in Natural Language Processing, and so we make some modifications to the \ac{ccg} formalism to apply it to Plan Recognition. Firstly, it is possible for intervening actions to occur which are not part of the current goal; they may be part of some other goal, or entirely irrelevant to the goals we are interested in. We therefore allow complex categories to find their arguments anywhere to the left (for leftward-applying categories) or to the right (for rightward-applying categories), rather than directly to the left or right.

Secondly, certain actions to achieve a goal may in certain cases be executed in any order with equal results. Although this could be expressed in the \ac{ccg} formalism, by including every possible permutation in the grammar, it is preferable to allow complex categories to take sets of arguments which may be provided in any order. This is denoted as $A/\{B, C\}$ for a complex category taking arguments $B$ and $C$ to the right in any order, and yielding $A$ [TODO: cite Hoffman].

\section{The ELEXIR Algorithm}

The ELEXIR algorithm [TODO: cite] provides several additional features to \ac{ccg}s used for plan recognition. \emph{Features} give first-order variable arguments to basic categories, which refer to labels in the domain. For example, \texttt{putAwayC(cup1)}. This allows categories in the domain to refer to many different potential objects, rather than have to repeat similar categories for each potential object in the domain. The same feature label refers to the same object in all basic categories within a complex category. When categories are combined with combinators, categories which were previously tested for equality must now be \emph{unified}, with unbound features in one bound to corresponding bound features in the other, and corresponding bound features tested for equality. \emph{States}, defined in terms of first-order predicates, are tracked through each explanation, each action updating the current state predicates. Assignments of categories to each action can be made conditional on certain predicates holding in the current state. \emph{Probabilities} for each explanation are computed, based on probabilities assigned in the input to categories and assignments of categories to actions. These probabilities can be made conditional on the current state predicates.

The ELEXIR algorithm restricts itself to three combinators: left application, right application, and right composition. It builds up a list of explanations by successively adding categories for new observations to existing explanations, and then applying any applicable combinators to pairs of categories in the explanation. Each input action can modify the state predicates, and can be assigned one or more categories, depending on whether preconditions on their applicability are satisfied by the current state. Combinators are applicable if the relevant arguments or results can be unified. When a new category is added for an observed action, at least one new explanation is produced for each existing one: the one in which no combinators are applied.

\section{Concurrency Techniques}

We can split our discussion of concurrency techniques between blocking and non-blocking methods.

Blocking methods include those involving mutual exclusion locks (mutexes), semaphores, condition variables, and other constructs where a thread may block on a concurrency construct indefinitely. [TODO: expand further on each of these?]

Non-blocking methods several possible levels of guarantee. \emph{Wait-free} algorithms guarantee that threads will always complete in a finite number of steps, but are usually expensive. \emph{Lock-free} algorithms only guarantee that some thread will always be making progress, and can be much cheaper than wait-free algorithms. Non-blocking algorithms depend on lower-level primitives than blocking ones; most commonly, the \ac{cas} operation available on modern processors along with memory fences which provide some guarantees with respect to the ordering of memory accesses. Non-blocking algorithms are harder to implement and reason about than blocking methods, but can provide improved performance and lower overhead in return.

[TODO describe performance issues: waiting, kernel switches, busy waits, starvation, priority inversion, cache contention and false sharing?]
[TODO describe correctness issues: missed wake-ups, deadlocks, static/dynamic hazards, ABA problem, ...?]

\section{Multicore Task Scheduling Techniques}

Many problems can be broken down into individual units of work to be executed across several threads. These tasks may have dependencies on other tasks which they require to be complete before they can begin; and they may generate new tasks to be scheduled. There are many possible techniques for scheduling work across threads, with different overheads and complexities of implementation. The lower the overhead of scheduling, the more fine-grained we can make the tasks, and the better we can load-balance to ensure all threads perform useful work.

One of the simplest possible implementations uses a single global queue for all tasks. Access to the queue is guarded by a global lock. Tasks are enqueued at the tail of the queue, and dequeued at the head. This implementation guarantees that a thread will be able to find a task if one is available, but the contention by all threads on a single global lock means that the overhead of this method is potentially high especially for short-lived tasks.

A potentially better implementation uses one queue of work for each worker thread. This reduces contention when threads are requesting work from the queues. In order to ensure that work is distributed to all threads, the main thread must periodically distribute work to all the threads, and worker threads must feed any new tasks to be scheduled back to the main thread. While this reduces contention, worker threads may not be able to find tasks to execute if they are available but the main thread has not scheduled them yet.

The final case we will examine is a lock-free work-stealing scheduler, with one queue for each worker thread. In a work-stealing scheduler, threads which find their own queue empty will select a random \emph{victim} thread and attempt to steal a task from their queue. New tasks will be added to the thread's own queue. In this way, work will be distributed between threads, but most of the accesses will be uncontended accesses to a thread's own queue. This can be implemented lock-free and has potentially the lowest contention of the methods mentioned here while balancing work well across threads.

[TODO cite that XB360/dev conf talk on task queues?]

\section{Applying Task Scheduling Techniques to ELEXIR}

In the ELEXIR algorithm, each existing explanation is processed independently when a new action is added to the observations. Each explanation generates one or more new explanations, with no dependencies on other computations. This makes ELEXIR well suited to parallelising using task scheduling. We can assign to each task some number of explanations, depending on the overhead of the method we are using.

[TODO Work out the complexity (Could go in appendix)]
[TODO Work out the perfect $T_1$ to $T_\infty$ speedup (Could go in appendix)]
