\chapter{Background}

In this section, we discuss the background for the project and provide context for the work performed.

\section{Plan Recognition}

Plan Recognition looks at a sequence of actions by an agent, and attempts to deduce that agent's goals. There are many possible situations where it is desirable to recognise the intentions of an agent from its observed actions. In network intrusion detection, we wish to identify any actions which could correspond to an attack, hidden in a large number of benign actions by users, in real time\cite{bib:netsec}. In human-robot interaction, we wish a human and a robot to work together on a task. If the robot partner can observe the actions of the human partner, and from them understand what the human partner is currently trying to achieve, it an anticipate the needs of the human partner, and assist or offer advice\cite{bib:jast}. In assisted care scenarios, a robot can again anticipate the needs of the human being assisted; it can also recognise unusual behaviour which would necessitate emergency response\cite{bib:assistive}.

Since Plan Recognition takes symbolic input, some amount of preprocessing of the input may be required. For network intrusion detection, this could be a simple filter on system logs. For more complex scenarios, Plan Recognition could tie in to a system which takes video and other real-time feeds and classifies the actions taken by humans using machine learning techniques.

Some Plan Recognition techniques use a formal grammar to define actions and plans to be recognised. In this project, we use two grammars crafted by hand to fit different problem domains. The formal grammars are defined in the language of \emph{Combinatory Categorial Grammars}, discussed below.

\section{Combinatory Categorial Grammars}

The problem of plan recognition is very similar to the problem of parsing, where we analyse a sequence of tokens and produce a syntax tree according to a formal grammar; so it is natural to consider the applicability of parsing techniques to this problem. When parsing computer languages such as XML or C++, ambiguity is undesirable. An input text should admit to either a single interpretation or no interpretation at all, and it is the task of the language designer to resolve any possible ambiguities in the grammar.

When parsing a sequence of actions into possible explanations, a degree of ambiguity is unavoidable; for example, if we observe a truck driving from Edinburgh to Newcastle, it may be making a delivery locally in Newcastle; or it may be driving there in its way to York. Grammars and parsers for natural language must deal with potential ambiguity and multiple possible parses in human language. Therefore, we can expect parsing techniques from natural language processing may be more amenable to being adapted to the problem of Plan Recognition than parsing techniques for computer languages.

Categorial Grammars\cite{bib:ccg} assign \emph{categories} to each input token or symbol, and combine them according to simple rules. \emph{Basic} categories correspond to lexical classes such as ``identifier'' in a computer language or ``noun'' in a natural language. We denote basic categories by atomic terms such as $A$. \emph{Complex} categories are akin to functions over categories: they look for arguments to their left (denoted $\backslash$) or right (denoted $/$) in the sequence of categories, and yield other categories as a result. A complex category taking an argument $A$ to its left and yielding $B$ is denoted $B\backslash A$; one taking the same argument to the right is denoted $B/A$. \footnote{Some sources denote the leftward example above as $A\backslash B$, preserving the ordering of the argument and result. For consistency we will always denote the arguments on the right and the results on the left.}

Taking a simple example from the Logistics domain:

\begin{lstlisting}
observations: [
  loadTruck(p0, t0, c1),
  driveTruck(t0, c1, c0),
  unloadTruck(p0, t0, c0)
];
\end{lstlisting}

We have three observations: package \texttt{p0} is loaded into truck \texttt{t0} at location \texttt{c1}, the truck drives to location \texttt{c0}, and the package is unloaded there. We then assign basic or complex categories to each observation:

\begin{lstlisting}
loadTruck(p0, t0, c1) :
  inTruckC(p0, t0)
driveTruck(t0, c1, c0) :
  (deliverPkgC(o, c0)/notInTruckC(o, t0))\inTruckC(obj, t0)o
unloadTruck(p0, t0, c0) :
  notInTruckC(p0, t0)
\end{lstlisting}


We can then combine the complex categories with their arguments and produce a parse. Combining the categories follows the rules: $ X\leftarrow X/Y,\; Y$ and $ X\leftarrow Y,\; X\backslash Y$. Note that package \texttt{o} is unbound until the complex category is unified with an argument.

\noindent\rule{\textwidth}{0.4pt}
{\ttfamily\scriptsize
$\underbrace{\text{inTruckC(p0, t0)},\quad \text{(deliverPkgC(o, c0)}/\text{notInTruckC(obj, t0))}\backslash{}\text{inTruckC(o, t0)}},\quad \text{notInTruckC(p0, t0)}$\\
$\hspace*{10em}\underbrace{\text{deliverPkgC(p0, c0)}/\text{notInTruckC(p0, t0)},\hspace*{9em}\text{notInTruckC(p0, t0)}}$\\
$\hspace*{24em} \text{deliverPkgC(p0, c0)}$
}\\
\noindent\rule{\textwidth}{0.4pt}

We reduce the input to an explanation consisting of a single basic category, \texttt{deliverPkgC(p0, c0)}.
\emph{Combinatory Categorial Grammars}\cite{bib:ccg} (CCGs) extend categorial grammars with new reductions corresponding to combinators in combinatory logic. Since complex categories can take arguments either to their left or right, each combinator has a leftward and a rightward version.

The leftward and rightward application combinators are the reductions already present in categorial grammars. The other two combinators commonly used in CCGs are composition and type-raising.

Composition corresponds to function composition. The rightward composition operation has the form: $X/Z \leftarrow X/Y,\;  Y/Z$. Type-raising transforms basic categories to complex categories. The rightward type-raising operation has the form: $T/(T\backslash X) \leftarrow X$.

\section{Applying CCGs to Plan Recognition}

In applying CCGs to Plan Recognition, we seek to assign (basic or complex) categories to the input sequence of actions, and by reductions using a set of combinators, produce a set of explanations for the input. Adjacency of grammar elements is not as crucial in Plan Recognition as in Natural Language Processing, and so we make some modifications to the CCG formalism to apply it to Plan Recognition.
Firstly, it is possible for intervening actions to occur which are not part of the current goal; they may be part of some other goal, or entirely irrelevant to the goals we are interested in. We therefore allow complex categories to find their arguments anywhere to the left (for leftward-applying categories) or to the right (for rightward-applying categories), rather than directly to the left or right.

Secondly, certain actions to achieve a goal may in certain cases be executed in any order with equal results. Although this could be expressed in the CCG formalism, by including every possible permutation in the grammar, it is preferable to allow complex categories to take sets of arguments which may be provided in any order. This is denoted as $A/\{B, C\}$ for a complex category taking arguments $B$ and $C$ to the right in any order, and yielding $A$\cite{bib:ccg2}.

\section{The ELEXIR Algorithm}

The ELEXIR algorithm\cite{bib:elexir} provides several additional features to CCGs used for plan recognition. \emph{Features} give first-order variable arguments to basic categories, which refer to labels in the domain. For example, \texttt{putAwayC(cup1)}. This allows categories in the domain to refer to many different potential objects, rather than have to repeat similar categories for each potential object in the domain. The same feature label refers to the same object in all basic categories within a complex category.
When categories are combined with combinators, categories which were previously tested for equality must now be \emph{unified}. When we unify two categories, if the same feature is bound in both of them, it must be bound to the same value; otherwise unification fails. If the same feature is bound in one of them and unbound in the other, the unbound feature takes the value of the bound feature. \emph{States}, defined in terms of first-order predicates, are tracked through each explanation, 
each 
action updating the current state predicates. Assignments of categories to each action can be made conditional on certain 
predicates holding in the current state. \emph{Probabilities} for each explanation are computed, based on probabilities assigned in the input to categories and assignments of categories to actions. These probabilities can be made conditional on the current state predicates.

The ELEXIR algorithm restricts itself to three combinators: left application, right application, and right composition. It builds up a list of explanations by successively adding categories for new observations to existing explanations, and then applying any applicable combinators to pairs of categories in the explanation.
Each input action can modify the state predicates, and can be assigned one or more categories, depending on whether preconditions on their applicability are satisfied by the current state. Combinators are applicable if the relevant arguments or results can be unified. When a new category is added for an observed action, at least one new explanation is produced for each existing one: the one in which no combinators are applied.

In the ELEXIR algorithm, each existing explanation is processed independently when a new action is added to the observations. For each existing explanation, the algorithm tries to apply the three combinators mentioned above between pairs of categories. Each successful application creates a new explanation; the case where no combinators are applied is also a valid new explanation. This process does not refer to any other existing explanations, and does not make modifications to the problem state outside the set of current explanations. This makes ELEXIR well suited to parallelising using task scheduling: depending on the overhead of the method we are using, we can create tasks containing some small or large number of explanations, and schedule these tasks to be executed independently across multiple threads. We now look at techniques for parallelism and how we might apply them to speed up this algorithm.

\section{Concurrency Techniques}

We can split our discussion of concurrency techniques between blocking and non-blocking methods. Blocking methods include those involving mutual exclusion locks (mutexes), semaphores, condition variables, and other constructs where a thread may block on a concurrency construct indefinitely. Mutexes are used to guard access to an object. Before accessing the object, threads must lock the mutex. Only one thread at a time is permitted to hold the mutex locked; other threads which attempt this will sleep until the lock becomes available.
This mechanism allows an object designed to be accessed in a single-thread manner, be safely accessed by multiple threads. Condition variables provide a way for threads to wait until a condition is satified by a shared object, for example for a queue to become non-empty. Threads which modify the object in such a way that the condition might now hold where it previously did not, should notify the condition variable, which will then wake the sleeping threads. These threads will 
then test whether the condition does indeed hold. Condition variables are often used in conjunction with mutexes, to guard access to the object being waited on.

Non-blocking methods offer several possible levels of guarantee. \emph{Wait-free} algorithms guarantee that threads will always complete in a finite number of steps, but are usually expensive. \emph{Lock-free} algorithms only guarantee that some thread will always be making progress, and can be much cheaper than wait-free algorithms. Non-blocking algorithms depend on lower-level primitives than blocking ones; most commonly, the \emph{Compare-And-Swap} (CAS) operation available on modern processors along with memory fences which provide some guarantees with respect to the ordering of memory accesses. Non-blocking algorithms are harder to implement and reason about than blocking methods, but can provide improved performance and lower overhead in return.

There are many potential performance issues in multi-threaded code. The two most relevant to this project are \emph{lock contention}, where threads spend a lot of time waiting on the same mutex for access to a shared object; and \emph{cache contention}, where threads concurrently access memory in the same cache line, causing delays as it is acquired by the caches of the processors the threads are executing on. Results from cachegrind allow us to discover potential cases of cache contention.

\section{Multicore Task Scheduling Techniques} \label{sec:sched}

Many problems can be broken down into individual units of work to be executed across several threads\cite{bib:aomp}. These tasks may have dependencies on other tasks which they require to be complete before they can begin; and they may generate new tasks to be scheduled. There are many possible techniques for scheduling work across threads, with different overheads and complexities of implementation. The lower the overhead of scheduling, the more fine-grained we can make the tasks, and the better we can load-balance to ensure all threads perform useful work. We will discuss several possible techniques for job scheduling on multicore machines, which we will evaluate in parallelising the ELEXIR algorithm.

\begin{description}
\item[Global Queue] One of the simplest possible implementations uses a single global queue for all tasks. Access to the queue is guarded by a global lock. Tasks are enqueued at the tail of the queue, and dequeued at the head. This implementation guarantees that a thread will be able to find a task if one is available, but the contention by all threads on a single global lock means that the overhead of this method is potentially high especially for short-lived tasks.

\item[Blocking Queues] A potentially better implementation uses one queue of work for each worker thread. This reduces contention when threads are requesting work from the queues. In order to ensure that work is distributed to all threads, the main thread must periodically distribute work to all the threads, and worker threads must feed any new tasks to be scheduled back to the main thread. While this reduces contention, worker threads may not be able to find tasks to execute if they are available but the main thread has not scheduled them yet.

\item[Work-Stealing Queues] The final case we will examine is a lock-free work-stealing scheduler, with one queue for each worker thread. In a work-stealing scheduler, threads which find their own queue empty will select a random \emph{victim} thread and attempt to steal a task from their queue. New tasks created by a thread will be added to the thread's own queue. In this way, work will be distributed between threads, but most of the accesses will be uncontended accesses to a thread's own queue. This can be implemented lock-free and has potentially the lowest contention of the methods mentioned here while balancing work well across threads.
\end{description}
